{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10d5da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec1688f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  Love encompasses a range of strong and positive emotional and mental states, from the most sublime virtue or good habit, \n",
      "\n",
      "The Last Line:  the feelings involved makes love unusually difficult to consistently define, compared to other emotional states.\n"
     ]
    }
   ],
   "source": [
    "file = open(\"data1.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593c8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96112731",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aff6520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love encompasses a range of strong and positive emotional and mental states, from the most sublime virtue or good habit,  the deepest interpersonal affection, to the simplest pleasure. An example of this range of meanings is that the  love of a mother differs from the love of a spouse, which differs from the love for food. Most commonly,  love refers to a feeling of a strong attraction and emotion'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2bcdaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love encompasses a range of strong and positive emotional and mental states  from the most sublime virtue or good habit   the deepest interpersonal affection  to the simplest pleasure  An example of this range of meanings is that the  love of a mother differs from the love of a spouse  which differs from the love for food  Most commonly   love refers to a feeling of a strong attraction and emotional attachment   Love is considered to be both positive and negative  with its virtue representing hu'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e572c0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love encompasses a range of strong and positive emotional mental states, from the most sublime virtue or good habit, deepest interpersonal affection, to simplest pleasure. An example this meanings is that love mother differs spouse, which for food. Most commonly, refers feeling attraction attachment. considered be both negative, with its representing human kindness, compassion, as \"the unselfish loyal benevolent concern another\" vice moral flaw, akin vanity, selfishness, amour-propre, egotism, p'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in q:\n",
    "        q.append(i)\n",
    "        \n",
    "data = ' '.join(q)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16f6d7",
   "metadata": {},
   "source": [
    "# Tockenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7250a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 12, 13, 14, 15, 16, 2, 17, 18, 19, 3, 20, 4, 7, 21]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10be2f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79921bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1, 12],\n",
       "       [12, 13],\n",
       "       [13, 14],\n",
       "       [14, 15],\n",
       "       [15, 16]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ab4048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1058205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 1 12 13 14 15]\n",
      "The responses are:  [12 13 14 15 16]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4796a0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0, 12, 13])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len = max([len(X) for x in sequences])\n",
    "input_sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "816114b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb7f5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=1))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ae76bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 100)            22400     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 100)            80400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 224)               22624     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 215,924\n",
      "Trainable params: 215,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb68dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a6633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62faf5c8",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "438c84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6527db6",
   "metadata": {},
   "source": [
    "# Compile The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca16d39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kasirai\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ed6d5",
   "metadata": {},
   "source": [
    "# Fitting The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b1ca193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/4 [==============>...............] - ETA: 0s - loss: 5.4119 - accuracy: 0.0078     \n",
      "Epoch 1: loss improved from inf to 5.41255, saving model to nextword1.h5\n",
      "4/4 [==============================] - 10s 363ms/step - loss: 5.4125 - accuracy: 0.0082 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.4109 - accuracy: 0.0312\n",
      "Epoch 2: loss improved from 5.41255 to 5.41122, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 5.4112 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.4108 - accuracy: 0.0156\n",
      "Epoch 3: loss improved from 5.41122 to 5.41048, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 125ms/step - loss: 5.4105 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.4098 - accuracy: 0.0312\n",
      "Epoch 4: loss improved from 5.41048 to 5.40961, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 156ms/step - loss: 5.4096 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.4094 - accuracy: 0.0156\n",
      "Epoch 5: loss improved from 5.40961 to 5.40873, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 5.4087 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.4064 - accuracy: 0.0469\n",
      "Epoch 6: loss improved from 5.40873 to 5.40766, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 115ms/step - loss: 5.4077 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.4067 - accuracy: 0.0312\n",
      "Epoch 7: loss improved from 5.40766 to 5.40635, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 207ms/step - loss: 5.4064 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.4058 - accuracy: 0.0000e+00\n",
      "Epoch 8: loss improved from 5.40635 to 5.40460, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 5.4046 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.4039 - accuracy: 0.0000e+00\n",
      "Epoch 9: loss improved from 5.40460 to 5.40246, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 137ms/step - loss: 5.4025 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.4017 - accuracy: 0.0156\n",
      "Epoch 10: loss improved from 5.40246 to 5.39957, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 252ms/step - loss: 5.3996 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.3988 - accuracy: 0.0156\n",
      "Epoch 11: loss improved from 5.39957 to 5.39539, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 163ms/step - loss: 5.3954 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.3921 - accuracy: 0.0000e+00\n",
      "Epoch 12: loss improved from 5.39539 to 5.38930, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 5.3893 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.3839 - accuracy: 0.0156\n",
      "Epoch 13: loss improved from 5.38930 to 5.38079, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 5.3808 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.3781 - accuracy: 0.0156\n",
      "Epoch 14: loss improved from 5.38079 to 5.36679, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 139ms/step - loss: 5.3668 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.3532 - accuracy: 0.0312\n",
      "Epoch 15: loss improved from 5.36679 to 5.34620, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 150ms/step - loss: 5.3462 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.3209 - accuracy: 0.0156\n",
      "Epoch 16: loss improved from 5.34620 to 5.31294, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 5.3129 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.3156 - accuracy: 0.0312\n",
      "Epoch 17: loss improved from 5.31294 to 5.26286, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 164ms/step - loss: 5.2629 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.2519 - accuracy: 0.0000e+00\n",
      "Epoch 18: loss improved from 5.26286 to 5.18849, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 164ms/step - loss: 5.1885 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.1149 - accuracy: 0.0156\n",
      "Epoch 19: loss improved from 5.18849 to 5.09167, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 5.0917 - accuracy: 0.0247 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.9974 - accuracy: 0.0312\n",
      "Epoch 20: loss improved from 5.09167 to 5.00301, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 252ms/step - loss: 5.0030 - accuracy: 0.0288 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.9784 - accuracy: 0.0156\n",
      "Epoch 21: loss improved from 5.00301 to 4.94703, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 210ms/step - loss: 4.9470 - accuracy: 0.0288 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.9571 - accuracy: 0.0312\n",
      "Epoch 22: loss improved from 4.94703 to 4.86347, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 119ms/step - loss: 4.8635 - accuracy: 0.0329 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.8571 - accuracy: 0.0156\n",
      "Epoch 23: loss improved from 4.86347 to 4.78244, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 4.7824 - accuracy: 0.0329 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.7461 - accuracy: 0.0781\n",
      "Epoch 24: loss improved from 4.78244 to 4.68649, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 4.6865 - accuracy: 0.0453 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.4566 - accuracy: 0.0469\n",
      "Epoch 25: loss improved from 4.68649 to 4.58169, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 4.5817 - accuracy: 0.0453 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.6588 - accuracy: 0.0469\n",
      "Epoch 26: loss improved from 4.58169 to 4.45474, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 148ms/step - loss: 4.4547 - accuracy: 0.0494 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.3227 - accuracy: 0.0625\n",
      "Epoch 27: loss improved from 4.45474 to 4.31322, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 252ms/step - loss: 4.3132 - accuracy: 0.0494 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.0829 - accuracy: 0.0625\n",
      "Epoch 28: loss improved from 4.31322 to 4.15671, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 216ms/step - loss: 4.1567 - accuracy: 0.0617 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 4.0922 - accuracy: 0.0938\n",
      "Epoch 29: loss improved from 4.15671 to 3.98200, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 3.9820 - accuracy: 0.0947 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 3.6944 - accuracy: 0.1719\n",
      "Epoch 30: loss improved from 3.98200 to 3.79131, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 266ms/step - loss: 3.7913 - accuracy: 0.1070 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 3.5473 - accuracy: 0.1719\n",
      "Epoch 31: loss improved from 3.79131 to 3.58607, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 3.5861 - accuracy: 0.1358 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 3.3785 - accuracy: 0.1875\n",
      "Epoch 32: loss improved from 3.58607 to 3.37011, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 3.3701 - accuracy: 0.1770 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 3.1587 - accuracy: 0.2344\n",
      "Epoch 33: loss improved from 3.37011 to 3.15062, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 221ms/step - loss: 3.1506 - accuracy: 0.2263 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 2.8265 - accuracy: 0.2969\n",
      "Epoch 34: loss improved from 3.15062 to 2.93361, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 127ms/step - loss: 2.9336 - accuracy: 0.2757 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 2.7698 - accuracy: 0.4062\n",
      "Epoch 35: loss improved from 2.93361 to 2.72109, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 275ms/step - loss: 2.7211 - accuracy: 0.3457 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 2.4820 - accuracy: 0.4219\n",
      "Epoch 36: loss improved from 2.72109 to 2.51475, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 299ms/step - loss: 2.5148 - accuracy: 0.3827 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 2.3155 - accuracy: 0.4403\n",
      "Epoch 37: loss improved from 2.51475 to 2.31551, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 242ms/step - loss: 2.3155 - accuracy: 0.4403 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 2.1251 - accuracy: 0.5781\n",
      "Epoch 38: loss improved from 2.31551 to 2.13031, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 217ms/step - loss: 2.1303 - accuracy: 0.5062 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 2.0550 - accuracy: 0.4844\n",
      "Epoch 39: loss improved from 2.13031 to 1.96252, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 162ms/step - loss: 1.9625 - accuracy: 0.5514 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.7415 - accuracy: 0.7031\n",
      "Epoch 40: loss improved from 1.96252 to 1.80026, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 155ms/step - loss: 1.8003 - accuracy: 0.6214 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.6600 - accuracy: 0.7031\n",
      "Epoch 41: loss improved from 1.80026 to 1.65433, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 227ms/step - loss: 1.6543 - accuracy: 0.6626 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.5001 - accuracy: 0.6875\n",
      "Epoch 42: loss improved from 1.65433 to 1.51740, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 308ms/step - loss: 1.5174 - accuracy: 0.6996 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.3948 - accuracy: 0.7656\n",
      "Epoch 43: loss improved from 1.51740 to 1.39949, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 316ms/step - loss: 1.3995 - accuracy: 0.7449 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.2267 - accuracy: 0.7969\n",
      "Epoch 44: loss improved from 1.39949 to 1.29298, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 221ms/step - loss: 1.2930 - accuracy: 0.7737 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.3104 - accuracy: 0.7812\n",
      "Epoch 45: loss improved from 1.29298 to 1.19113, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 447ms/step - loss: 1.1911 - accuracy: 0.8313 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.1298 - accuracy: 0.8750\n",
      "Epoch 46: loss improved from 1.19113 to 1.10059, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 340ms/step - loss: 1.1006 - accuracy: 0.8354 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.9331 - accuracy: 0.9219\n",
      "Epoch 47: loss improved from 1.10059 to 1.02583, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 314ms/step - loss: 1.0258 - accuracy: 0.8395 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.9690 - accuracy: 0.8906\n",
      "Epoch 48: loss improved from 1.02583 to 0.95059, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 376ms/step - loss: 0.9506 - accuracy: 0.8807 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.9390 - accuracy: 0.8125\n",
      "Epoch 49: loss improved from 0.95059 to 0.88462, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 282ms/step - loss: 0.8846 - accuracy: 0.8807 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7665 - accuracy: 0.9219\n",
      "Epoch 50: loss improved from 0.88462 to 0.82411, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 313ms/step - loss: 0.8241 - accuracy: 0.8971 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7862 - accuracy: 0.9219\n",
      "Epoch 51: loss improved from 0.82411 to 0.76939, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 192ms/step - loss: 0.7694 - accuracy: 0.9012 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7133 - accuracy: 0.9062\n",
      "Epoch 52: loss improved from 0.76939 to 0.72104, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 207ms/step - loss: 0.7210 - accuracy: 0.9012 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6214 - accuracy: 0.9375\n",
      "Epoch 53: loss improved from 0.72104 to 0.66830, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 247ms/step - loss: 0.6683 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5973 - accuracy: 0.8750\n",
      "Epoch 54: loss improved from 0.66830 to 0.62872, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 191ms/step - loss: 0.6287 - accuracy: 0.8971 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5487 - accuracy: 0.9375\n",
      "Epoch 55: loss improved from 0.62872 to 0.59007, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 333ms/step - loss: 0.5901 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4954 - accuracy: 0.9844\n",
      "Epoch 56: loss improved from 0.59007 to 0.55194, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 342ms/step - loss: 0.5519 - accuracy: 0.9218 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4734 - accuracy: 0.9688\n",
      "Epoch 57: loss improved from 0.55194 to 0.52231, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 141ms/step - loss: 0.5223 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4776 - accuracy: 0.9375\n",
      "Epoch 58: loss improved from 0.52231 to 0.49527, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 210ms/step - loss: 0.4953 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4684 - accuracy: 0.9062\n",
      "Epoch 59: loss improved from 0.49527 to 0.46821, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 366ms/step - loss: 0.4682 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4656 - accuracy: 0.9062\n",
      "Epoch 60: loss improved from 0.46821 to 0.44365, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 284ms/step - loss: 0.4436 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3773 - accuracy: 0.9375\n",
      "Epoch 61: loss improved from 0.44365 to 0.42045, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 286ms/step - loss: 0.4205 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3876 - accuracy: 0.9375\n",
      "Epoch 62: loss improved from 0.42045 to 0.40269, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 304ms/step - loss: 0.4027 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3935 - accuracy: 0.9219\n",
      "Epoch 63: loss improved from 0.40269 to 0.38218, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 315ms/step - loss: 0.3822 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4400 - accuracy: 0.8750\n",
      "Epoch 64: loss improved from 0.38218 to 0.36870, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 268ms/step - loss: 0.3687 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3188 - accuracy: 0.9062\n",
      "Epoch 65: loss improved from 0.36870 to 0.35183, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 269ms/step - loss: 0.3518 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4012 - accuracy: 0.9062\n",
      "Epoch 66: loss improved from 0.35183 to 0.33922, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 158ms/step - loss: 0.3392 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3257 - accuracy: 0.9062\n",
      "Epoch 67: loss improved from 0.33922 to 0.32669, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 223ms/step - loss: 0.3267 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2983 - accuracy: 0.9375\n",
      "Epoch 68: loss improved from 0.32669 to 0.31638, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.3164 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2737 - accuracy: 0.9375\n",
      "Epoch 69: loss improved from 0.31638 to 0.30637, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 446ms/step - loss: 0.3064 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2935 - accuracy: 0.9375\n",
      "Epoch 70: loss improved from 0.30637 to 0.29505, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 271ms/step - loss: 0.2951 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2858 - accuracy: 0.9375\n",
      "Epoch 71: loss improved from 0.29505 to 0.28590, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 250ms/step - loss: 0.2859 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2734 - accuracy: 0.9219\n",
      "Epoch 72: loss improved from 0.28590 to 0.27840, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 354ms/step - loss: 0.2784 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3081 - accuracy: 0.9062\n",
      "Epoch 73: loss improved from 0.27840 to 0.27211, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 480ms/step - loss: 0.2721 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3078 - accuracy: 0.9062\n",
      "Epoch 74: loss improved from 0.27211 to 0.26481, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 184ms/step - loss: 0.2648 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2022 - accuracy: 0.9531\n",
      "Epoch 75: loss improved from 0.26481 to 0.25885, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.2589 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2323 - accuracy: 0.9062\n",
      "Epoch 76: loss improved from 0.25885 to 0.25391, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 292ms/step - loss: 0.2539 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2761 - accuracy: 0.9219\n",
      "Epoch 77: loss improved from 0.25391 to 0.24907, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 293ms/step - loss: 0.2491 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2333 - accuracy: 0.9219\n",
      "Epoch 78: loss improved from 0.24907 to 0.24177, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 140ms/step - loss: 0.2418 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1455 - accuracy: 0.9844\n",
      "Epoch 79: loss improved from 0.24177 to 0.23909, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 389ms/step - loss: 0.2391 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2944 - accuracy: 0.8906\n",
      "Epoch 80: loss improved from 0.23909 to 0.23758, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 206ms/step - loss: 0.2376 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2454 - accuracy: 0.9219\n",
      "Epoch 81: loss improved from 0.23758 to 0.23014, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 236ms/step - loss: 0.2301 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2673 - accuracy: 0.8594\n",
      "Epoch 82: loss improved from 0.23014 to 0.22799, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 257ms/step - loss: 0.2280 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2693 - accuracy: 0.9062\n",
      "Epoch 83: loss improved from 0.22799 to 0.22628, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 281ms/step - loss: 0.2263 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2167 - accuracy: 0.9219\n",
      "Epoch 84: loss improved from 0.22628 to 0.22019, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 308ms/step - loss: 0.2202 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2313 - accuracy: 0.9062\n",
      "Epoch 85: loss improved from 0.22019 to 0.21909, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 124ms/step - loss: 0.2191 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2743 - accuracy: 0.9062\n",
      "Epoch 86: loss improved from 0.21909 to 0.21228, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 246ms/step - loss: 0.2123 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2620 - accuracy: 0.8906\n",
      "Epoch 87: loss improved from 0.21228 to 0.21124, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 352ms/step - loss: 0.2112 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2276 - accuracy: 0.9062\n",
      "Epoch 88: loss improved from 0.21124 to 0.21019, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 488ms/step - loss: 0.2102 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2073 - accuracy: 0.9219\n",
      "Epoch 89: loss improved from 0.21019 to 0.20507, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 236ms/step - loss: 0.2051 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1916 - accuracy: 0.9219\n",
      "Epoch 90: loss did not improve from 0.20507\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 0.2060 - accuracy: 0.9012 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1636 - accuracy: 0.9375\n",
      "Epoch 91: loss did not improve from 0.20507\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2064 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1724 - accuracy: 0.9531\n",
      "Epoch 92: loss improved from 0.20507 to 0.20148, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 327ms/step - loss: 0.2015 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1998 - accuracy: 0.9219\n",
      "Epoch 93: loss improved from 0.20148 to 0.19824, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 193ms/step - loss: 0.1982 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1709 - accuracy: 0.9375\n",
      "Epoch 94: loss improved from 0.19824 to 0.19655, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 199ms/step - loss: 0.1965 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2756 - accuracy: 0.8438\n",
      "Epoch 95: loss did not improve from 0.19655\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1995 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1638 - accuracy: 0.9219\n",
      "Epoch 96: loss improved from 0.19655 to 0.19534, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 321ms/step - loss: 0.1953 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1622 - accuracy: 0.9531\n",
      "Epoch 97: loss improved from 0.19534 to 0.19292, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 194ms/step - loss: 0.1929 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2052 - accuracy: 0.9219\n",
      "Epoch 98: loss improved from 0.19292 to 0.19150, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 196ms/step - loss: 0.1915 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1917 - accuracy: 0.9219\n",
      "Epoch 99: loss improved from 0.19150 to 0.18985, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 154ms/step - loss: 0.1898 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1377 - accuracy: 0.9375\n",
      "Epoch 100: loss improved from 0.18985 to 0.18931, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 259ms/step - loss: 0.1893 - accuracy: 0.9012 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1184 - accuracy: 0.9531\n",
      "Epoch 101: loss improved from 0.18931 to 0.18824, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 0.1882 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1473 - accuracy: 0.9375\n",
      "Epoch 102: loss improved from 0.18824 to 0.18617, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 193ms/step - loss: 0.1862 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2173 - accuracy: 0.8750\n",
      "Epoch 103: loss did not improve from 0.18617\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1875 - accuracy: 0.8930 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1663 - accuracy: 0.9062\n",
      "Epoch 104: loss improved from 0.18617 to 0.18613, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 331ms/step - loss: 0.1861 - accuracy: 0.9012 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1446 - accuracy: 0.9375\n",
      "Epoch 105: loss improved from 0.18613 to 0.18173, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 314ms/step - loss: 0.1817 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1776 - accuracy: 0.9219\n",
      "Epoch 106: loss improved from 0.18173 to 0.18117, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 286ms/step - loss: 0.1812 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2057 - accuracy: 0.8906\n",
      "Epoch 107: loss did not improve from 0.18117\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.1828 - accuracy: 0.8971 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1988 - accuracy: 0.9062\n",
      "Epoch 108: loss improved from 0.18117 to 0.18052, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 221ms/step - loss: 0.1805 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1229 - accuracy: 0.9375\n",
      "Epoch 109: loss improved from 0.18052 to 0.17808, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 313ms/step - loss: 0.1781 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1749 - accuracy: 0.9375\n",
      "Epoch 110: loss did not improve from 0.17808\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1811 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1774 - accuracy: 0.9219\n",
      "Epoch 111: loss did not improve from 0.17808\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1814 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2622 - accuracy: 0.8750\n",
      "Epoch 112: loss improved from 0.17808 to 0.17782, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 301ms/step - loss: 0.1778 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1934 - accuracy: 0.9219\n",
      "Epoch 113: loss improved from 0.17782 to 0.17635, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 301ms/step - loss: 0.1764 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2148 - accuracy: 0.8750\n",
      "Epoch 114: loss did not improve from 0.17635\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.1780 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1840 - accuracy: 0.9062\n",
      "Epoch 115: loss did not improve from 0.17635\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1765 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1900 - accuracy: 0.9062\n",
      "Epoch 116: loss improved from 0.17635 to 0.17335, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 297ms/step - loss: 0.1734 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1666 - accuracy: 0.9062\n",
      "Epoch 117: loss improved from 0.17335 to 0.17258, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 137ms/step - loss: 0.1726 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1149 - accuracy: 0.9531\n",
      "Epoch 118: loss did not improve from 0.17258\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1747 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1642 - accuracy: 0.9062\n",
      "Epoch 119: loss improved from 0.17258 to 0.17153, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 284ms/step - loss: 0.1715 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2210 - accuracy: 0.9062\n",
      "Epoch 120: loss did not improve from 0.17153\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1727 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2030 - accuracy: 0.9375\n",
      "Epoch 121: loss did not improve from 0.17153\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.1735 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2037 - accuracy: 0.9062\n",
      "Epoch 122: loss improved from 0.17153 to 0.17020, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.1702 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2409 - accuracy: 0.9062\n",
      "Epoch 123: loss did not improve from 0.17020\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1709 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1220 - accuracy: 0.9531\n",
      "Epoch 124: loss did not improve from 0.17020\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1710 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2140 - accuracy: 0.8906\n",
      "Epoch 125: loss improved from 0.17020 to 0.16828, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 0.1683 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1219 - accuracy: 0.9375\n",
      "Epoch 126: loss improved from 0.16828 to 0.16815, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 230ms/step - loss: 0.1681 - accuracy: 0.9053 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1084 - accuracy: 0.9531\n",
      "Epoch 127: loss improved from 0.16815 to 0.16805, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 337ms/step - loss: 0.1681 - accuracy: 0.9012 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1936 - accuracy: 0.8906\n",
      "Epoch 128: loss improved from 0.16805 to 0.16770, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 236ms/step - loss: 0.1677 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1440 - accuracy: 0.9375\n",
      "Epoch 129: loss did not improve from 0.16770\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.1680 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1035 - accuracy: 0.9531\n",
      "Epoch 130: loss improved from 0.16770 to 0.16716, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 145ms/step - loss: 0.1672 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1478 - accuracy: 0.9375\n",
      "Epoch 131: loss did not improve from 0.16716\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1672 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1779 - accuracy: 0.9219\n",
      "Epoch 132: loss improved from 0.16716 to 0.16475, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 0.1647 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1975 - accuracy: 0.9219\n",
      "Epoch 133: loss improved from 0.16475 to 0.16436, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 0.1644 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1366 - accuracy: 0.9062\n",
      "Epoch 134: loss did not improve from 0.16436\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.1666 - accuracy: 0.9095 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1904 - accuracy: 0.8906\n",
      "Epoch 135: loss did not improve from 0.16436\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.1660 - accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1616 - accuracy: 0.9062\n",
      "Epoch 136: loss did not improve from 0.16436\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.1661 - accuracy: 0.9136 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1357 - accuracy: 0.9375\n",
      "Epoch 137: loss improved from 0.16436 to 0.15992, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 357ms/step - loss: 0.1599 - accuracy: 0.9177 - lr: 2.0000e-04\n",
      "Epoch 138/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1020 - accuracy: 0.9531\n",
      "Epoch 138: loss improved from 0.15992 to 0.15906, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 402ms/step - loss: 0.1591 - accuracy: 0.9218 - lr: 2.0000e-04\n",
      "Epoch 139/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2422 - accuracy: 0.8438\n",
      "Epoch 139: loss did not improve from 0.15906\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.1591 - accuracy: 0.9136 - lr: 2.0000e-04\n",
      "Epoch 140/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1849 - accuracy: 0.8906\n",
      "Epoch 140: loss improved from 0.15906 to 0.15823, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.1582 - accuracy: 0.9177 - lr: 2.0000e-04\n",
      "Epoch 141/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1482 - accuracy: 0.9375\n",
      "Epoch 141: loss did not improve from 0.15823\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1585 - accuracy: 0.9177 - lr: 2.0000e-04\n",
      "Epoch 142/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0922 - accuracy: 0.9531\n",
      "Epoch 142: loss improved from 0.15823 to 0.15788, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 352ms/step - loss: 0.1579 - accuracy: 0.9136 - lr: 2.0000e-04\n",
      "Epoch 143/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1983 - accuracy: 0.8906\n",
      "Epoch 143: loss did not improve from 0.15788\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.1581 - accuracy: 0.9136 - lr: 2.0000e-04\n",
      "Epoch 144/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2074 - accuracy: 0.8750\n",
      "Epoch 144: loss did not improve from 0.15788\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.1580 - accuracy: 0.9095 - lr: 2.0000e-04\n",
      "Epoch 145/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1464 - accuracy: 0.9062\n",
      "Epoch 145: loss did not improve from 0.15788\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1580 - accuracy: 0.9053 - lr: 2.0000e-04\n",
      "Epoch 146/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1986 - accuracy: 0.8906\n",
      "Epoch 146: loss improved from 0.15788 to 0.15733, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 213ms/step - loss: 0.1573 - accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Epoch 147/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1964 - accuracy: 0.8906\n",
      "Epoch 147: loss improved from 0.15733 to 0.15701, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 299ms/step - loss: 0.1570 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 148/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1904 - accuracy: 0.8906\n",
      "Epoch 148: loss did not improve from 0.15701\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 0.1572 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 149/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1578 - accuracy: 0.9062\n",
      "Epoch 149: loss did not improve from 0.15701\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.1573 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 150/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1914 - accuracy: 0.8906\n",
      "Epoch 150: loss did not improve from 0.15701\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1571 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 151/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1212 - accuracy: 0.9219\n",
      "Epoch 151: loss did not improve from 0.15701\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1571 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 152/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1933 - accuracy: 0.9375\n",
      "Epoch 152: loss improved from 0.15701 to 0.15690, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 324ms/step - loss: 0.1569 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 153/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0945 - accuracy: 0.9688\n",
      "Epoch 153: loss improved from 0.15690 to 0.15686, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 283ms/step - loss: 0.1569 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 154/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1389 - accuracy: 0.9375\n",
      "Epoch 154: loss improved from 0.15686 to 0.15683, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 246ms/step - loss: 0.1568 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 155/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1531 - accuracy: 0.9219\n",
      "Epoch 155: loss did not improve from 0.15683\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1569 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 156/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1272 - accuracy: 0.9375\n",
      "Epoch 156: loss did not improve from 0.15683\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1570 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 157/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1350 - accuracy: 0.9375\n",
      "Epoch 157: loss improved from 0.15683 to 0.15681, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 373ms/step - loss: 0.1568 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 158/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1868 - accuracy: 0.9219\n",
      "Epoch 158: loss did not improve from 0.15681\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.1568 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 159/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1894 - accuracy: 0.8906\n",
      "Epoch 159: loss did not improve from 0.15681\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.1568 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 160/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1567 - accuracy: 0.9219\n",
      "Epoch 160: loss improved from 0.15681 to 0.15670, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 236ms/step - loss: 0.1567 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 161/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2148 - accuracy: 0.8750\n",
      "Epoch 161: loss did not improve from 0.15670\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1567 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 162/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1676 - accuracy: 0.9531\n",
      "Epoch 162: loss did not improve from 0.15670\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.1570 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 163/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1798 - accuracy: 0.9062\n",
      "Epoch 163: loss did not improve from 0.15670\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.1569 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 164/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1733 - accuracy: 0.9062\n",
      "Epoch 164: loss improved from 0.15670 to 0.15663, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 260ms/step - loss: 0.1566 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 165/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0941 - accuracy: 0.9531\n",
      "Epoch 165: loss improved from 0.15663 to 0.15646, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 249ms/step - loss: 0.1565 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 166/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0875 - accuracy: 0.9688\n",
      "Epoch 166: loss improved from 0.15646 to 0.15634, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 196ms/step - loss: 0.1563 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 167/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0977 - accuracy: 0.9531\n",
      "Epoch 167: loss did not improve from 0.15634\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1565 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 168/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1124 - accuracy: 0.9531\n",
      "Epoch 168: loss did not improve from 0.15634\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1565 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 169/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2027 - accuracy: 0.8906\n",
      "Epoch 169: loss did not improve from 0.15634\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.1565 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 170/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2574 - accuracy: 0.8281\n",
      "Epoch 170: loss did not improve from 0.15634\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1565 - accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Epoch 171/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1324 - accuracy: 0.9375\n",
      "Epoch 171: loss did not improve from 0.15634\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.1564 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 172/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1299 - accuracy: 0.9375\n",
      "Epoch 172: loss did not improve from 0.15634\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.1564 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 173/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1272 - accuracy: 0.9375\n",
      "Epoch 173: loss improved from 0.15634 to 0.15633, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 209ms/step - loss: 0.1563 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 174/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1664 - accuracy: 0.9062\n",
      "Epoch 174: loss improved from 0.15633 to 0.15628, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 254ms/step - loss: 0.1563 - accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Epoch 175/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1264 - accuracy: 0.9375\n",
      "Epoch 175: loss improved from 0.15628 to 0.15624, saving model to nextword1.h5\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.1562 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 176/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1730 - accuracy: 0.8906\n",
      "Epoch 176: loss improved from 0.15624 to 0.15620, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 193ms/step - loss: 0.1562 - accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Epoch 177/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1632 - accuracy: 0.8906\n",
      "Epoch 177: loss did not improve from 0.15620\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.1562 - accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Epoch 178/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1651 - accuracy: 0.9062\n",
      "Epoch 178: loss improved from 0.15620 to 0.15607, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 293ms/step - loss: 0.1561 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 179/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1924 - accuracy: 0.9062\n",
      "Epoch 179: loss did not improve from 0.15607\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.1561 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 180/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0784 - accuracy: 0.9688\n",
      "Epoch 180: loss improved from 0.15607 to 0.15594, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 306ms/step - loss: 0.1559 - accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 181/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1756 - accuracy: 0.9219\n",
      "Epoch 181: loss improved from 0.15594 to 0.15584, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 228ms/step - loss: 0.1558 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 182/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1354 - accuracy: 0.9219\n",
      "Epoch 182: loss did not improve from 0.15584\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1559 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 183/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1733 - accuracy: 0.9375\n",
      "Epoch 183: loss did not improve from 0.15584\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1559 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 184/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1812 - accuracy: 0.8906\n",
      "Epoch 184: loss improved from 0.15584 to 0.15580, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 343ms/step - loss: 0.1558 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 185/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1600 - accuracy: 0.9375\n",
      "Epoch 185: loss did not improve from 0.15580\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1559 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 186/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1235 - accuracy: 0.9531\n",
      "Epoch 186: loss did not improve from 0.15580\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.1559 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 187/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1343 - accuracy: 0.9531\n",
      "Epoch 187: loss did not improve from 0.15580\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1558 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 188/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1304 - accuracy: 0.9375\n",
      "Epoch 188: loss did not improve from 0.15580\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1559 - accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Epoch 189/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1908 - accuracy: 0.8906\n",
      "Epoch 189: loss did not improve from 0.15580\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1558 - accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Epoch 190/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1900 - accuracy: 0.8906\n",
      "Epoch 190: loss improved from 0.15580 to 0.15572, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 342ms/step - loss: 0.1557 - accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Epoch 191/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1249 - accuracy: 0.9375\n",
      "Epoch 191: loss improved from 0.15572 to 0.15571, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 224ms/step - loss: 0.1557 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 192/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1205 - accuracy: 0.9219\n",
      "Epoch 192: loss did not improve from 0.15571\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1558 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 193/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1211 - accuracy: 0.9531\n",
      "Epoch 193: loss improved from 0.15571 to 0.15540, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 0.1554 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 194/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2341 - accuracy: 0.8906\n",
      "Epoch 194: loss did not improve from 0.15540\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1558 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 195/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1655 - accuracy: 0.9219\n",
      "Epoch 195: loss did not improve from 0.15540\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.1555 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 196/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1846 - accuracy: 0.8906\n",
      "Epoch 196: loss did not improve from 0.15540\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1555 - accuracy: 0.9053 - lr: 1.0000e-04\n",
      "Epoch 197/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1338 - accuracy: 0.9375\n",
      "Epoch 197: loss did not improve from 0.15540\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1555 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 198/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1654 - accuracy: 0.9062\n",
      "Epoch 198: loss improved from 0.15540 to 0.15538, saving model to nextword1.h5\n",
      "4/4 [==============================] - 1s 215ms/step - loss: 0.1554 - accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 199/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1201 - accuracy: 0.9375\n",
      "Epoch 199: loss did not improve from 0.15538\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.1554 - accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 200/200\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0807 - accuracy: 0.9375\n",
      "Epoch 200: loss did not improve from 0.15538\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.1555 - accuracy: 0.9053 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X, y, epochs=200, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69495d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "021b6cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7c819d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d0f27",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf7ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "beb026cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0, 12, 13])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len = max([len(X) for x in sequences])\n",
    "input_sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b296e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "model = load_model('nextword1.h5')\n",
    "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "746b2176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 242).\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Suggested next two word are :  ['may', 'cafuné']\n",
      "hello how may I help may cafuné\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Suggested next two word are :  ['postulated', 'involved']\n",
      "tell them he is busy  postulated involved\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Suggested next two word are :  ['primary', 'primary']\n",
      "he fell in love with primary primary\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Suggested next two word are :  ['primary', 'primary']\n",
      "he fell in love with primary primary\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Suggested next two word are :  ['flaw', 'flaw']\n",
      "she looks old but not flaw flaw\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while(True):\n",
    "    seed_text = input(\"Enter your line: \")\n",
    "  \n",
    "    if seed_text == \"0\":\n",
    "        print(\"Execution completed.....\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            next_words = 2\n",
    "            suggested_word = []\n",
    "            #temp = seed_text\n",
    "            for _ in range(next_words):\n",
    "                \n",
    "                token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "                #print(token_list)\n",
    "                token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "                predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "                output_word = \"\"\n",
    "                \n",
    "                for word, index in tokenizer.word_index.items():\n",
    "                    if index == predicted:\n",
    "                        output_word = word\n",
    "                        suggested_word.append(output_word)\n",
    "                        break\n",
    "                \n",
    "                seed_text += \" \" + output_word\n",
    "            print(\"Suggested next two word are : \",suggested_word)\n",
    "            #n = int(input())\n",
    "            \n",
    "            print(seed_text)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23e3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
